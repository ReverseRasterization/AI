{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b940825",
   "metadata": {},
   "source": [
    "# RFP: Betting on the Bachelor\n",
    "\n",
    "## Project Overview\n",
    "You are invited to submit a proposal that answers the following question:\n",
    "\n",
    "### Who will win season 29 of the Bachelor?\n",
    "\n",
    "*All proposals must be submitted by **1/15/25 at 11:59 PM**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a85f0",
   "metadata": {},
   "source": [
    "## Required Proposal Components\n",
    "\n",
    "### 1. Data Description\n",
    "In the code cell below, read in the data you plan on using to train and test your model. Call `info()` once you have read the data into a dataframe. Consider using some or all of the following sources:\n",
    "- [Scrape Fandom Wikis](https://bachelor-nation.fandom.com/wiki/The_Bachelor) or [the official Bachelor website]('https://bachelornation.com/shows/the-bachelor/')\n",
    "- [Ask ChatGPT to genereate it](https://chatgpt.com/)\n",
    "- [Read in csv files like this](https://www.kaggle.com/datasets/brianbgonz/the-bachelor-contestants?select=contestants.csv)\n",
    "\n",
    "*Note, a level 5 dataset contains at least 1000 rows of non-null data. A level 4 contains at least 500 rows of non-null data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c1688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into a dataframe\n",
    "# Don't forget to call info()!\n",
    "\n",
    "import pandas\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71a12ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing season 19\n",
      "Whitney Bischoff\n",
      "Becca Tilley\n",
      "Kaitlyn Bristowe\n",
      "Jade Roper\n",
      "Carly Waddell\n",
      "Britt Nilsson\n",
      "Megan Bell\n",
      "Kelsey Poe\n",
      "Ashley Iaconetti\n",
      "Mackenzie Deonigi\n",
      "Samantha Steffen\n",
      "Ashley Salter\n",
      "Juelia Kinney\n",
      "Nikki Delventhal\n",
      "Jillian Anderson\n",
      "Amber James\n",
      "Tracy Darakis\n",
      "Trina Scherenberg\n",
      "Alissa Giambrone\n",
      "Jordan Branch\n",
      "Error\n",
      "Kimberly Sherbach\n",
      "Error\n",
      "Tandra Steiner\n",
      "Tara Eddings\n",
      "Amanda Goerlitz\n",
      "Bo Stanley\n",
      "Brittany Fetkin\n",
      "Kara Wilson\n",
      "Michelle Davis\n",
      "Nicole Meacham\n",
      "Reegan Cornwell\n"
     ]
    }
   ],
   "source": [
    "contestants = {\n",
    "    \"Name\" : [],\n",
    "    \"Birth Year\" : [],\n",
    "    \"Hometown\" : [],\n",
    "    \"Occupation\" : [],\n",
    "    \"Season\" : [],\n",
    "    \"Eliminated\": []\n",
    "}\n",
    "\n",
    "def getHtml(url):\n",
    "    try:\n",
    "        html = requests.get(url)\n",
    "        html.raise_for_status()\n",
    "        return html\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to get url data: {url}\")\n",
    "        raise SystemExit\n",
    "    \n",
    "def tableImplContestants(soup, seasonNum):\n",
    "    global contestants\n",
    "\n",
    "    try:\n",
    "        table = soup.find('table', class_='article-table')\n",
    "        \n",
    "        if table:\n",
    "            table = table.find_all('tr')[1:]\n",
    "        else:\n",
    "            table = soup.find('table', class_='fandom-table').find_all('tr')[1:]\n",
    "    except Exception as e:\n",
    "        print(f\"Error on season {season}: {e}\")\n",
    "        return\n",
    "\n",
    "    for row in table:\n",
    "        try:\n",
    "            columns = row.find_all('td')\n",
    "\n",
    "            contestants['Name'].append(columns[0].text)\n",
    "            contestants['Hometown'].append(columns[2].text)\n",
    "            contestants['Occupation'].append(columns[3].text)\n",
    "\n",
    "            contestants['Season'].append(seasonNum)\n",
    "\n",
    "            eliminationStatus = columns[4].text\n",
    "\n",
    "            if (\"Winner\" in eliminationStatus):\n",
    "                contestants['Eliminated'].append(0)\n",
    "            else:\n",
    "                contestants['Eliminated'].append(1)\n",
    "\n",
    "            aElm =columns[0].find('a')\n",
    "            if aElm:\n",
    "                url = f'https://bachelor-nation.fandom.com{aElm.get('href')}'\n",
    "                \n",
    "                try:\n",
    "                    html = getHtml(url)\n",
    "                    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "                    birthData = soup.find('div', attrs={'data-source':'born'}).find('div').text.replace(',', '').split()\n",
    "\n",
    "                    if (birthData[0] != 'age'):\n",
    "                        contestants['Birth Year'].append(birthData[2])\n",
    "                    else:\n",
    "                        contestants['Birth Year'].append(birthData[1])\n",
    "                except:\n",
    "                    contestants['Birth Year'].append(columns[1].text)\n",
    "            else:\n",
    "                contestants['Birth Year'].append(columns[1].text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error for grabbing row on season {seasonNum}: {e}\")\n",
    "            continue\n",
    "\n",
    "def galleryImplContestants(soup, seasonNum):\n",
    "    print(\"Doing season\", seasonNum)\n",
    "\n",
    "    gallery = soup.find('div', class_='wikia-gallery wikia-gallery-caption-below wikia-gallery-position-center wikia-gallery-spacing-medium wikia-gallery-border-small wikia-gallery-captions-center wikia-gallery-caption-size-medium')\n",
    "\n",
    "    women = gallery.find_all('div', class_='wikia-gallery-item')\n",
    "    \n",
    "\n",
    "    for girl in women:\n",
    "        womenData = girl.find('div', class_='lightbox-caption')\n",
    "\n",
    "        print(womenData.find('a').text)\n",
    "\n",
    "        br_values = []\n",
    "\n",
    "        for br in womenData.find_all('br'):\n",
    "            try:\n",
    "                br_values.append(br.next_sibling.strip())\n",
    "            except:\n",
    "                print(\"Error\")\n",
    "\n",
    "\n",
    "        # print(br_values)\n",
    "        \n",
    "        contestants['Name'].append(womenData.find('a').text)\n",
    "        contestants['Hometown'].append(br_values[1])\n",
    "        contestants['Occupation'].append(br_values[2])\n",
    "        contestants['Season'].append(seasonNum)\n",
    "\n",
    "\n",
    "    \n",
    "        url = f'https://bachelor-nation.fandom.com{womenData.find('a').get('href')}'\n",
    "        \n",
    "        try:\n",
    "            html = getHtml(url)\n",
    "            soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "            birthData = soup.find('div', attrs={'data-source':'born'}).find('div').text.replace(',', '').split()\n",
    "\n",
    "            if (birthData[0] != 'age'):\n",
    "                contestants['Birth Year'].append(birthData[2])\n",
    "            else:\n",
    "                contestants['Birth Year'].append(birthData[1])\n",
    "        except:\n",
    "            contestants['Birth Year'].append(br_values[0])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bachelors = { # Note: Age is whenever the person was the bachelor\n",
    "    \"Name\" : [],\n",
    "    \"Birth Year\" : [],\n",
    "    \"Hometown\" : [],\n",
    "    \"Occupation\" : [],\n",
    "    \"Season\" : [] \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Get a list of all the seasons\n",
    "seasonList = []\n",
    "\n",
    "html = getHtml(\"https://bachelor-nation.fandom.com/wiki/Category:The_Bachelor_seasons\")\n",
    "\n",
    "soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "for season in soup.find_all('li', class_=\"category-page__member\")[1:]:\n",
    "    seasonList.append(\"/wiki/\" + season.text.strip())\n",
    "\n",
    "# Iterate through each season\n",
    "# ... first, find the bachelor, then get the data on said bachelor\n",
    "# ... then, get the contestants\n",
    "\n",
    "for season in seasonList:\n",
    "    seasonNum = season.replace(\"(\", \"\").split()[-1][:-1]\n",
    "\n",
    "\n",
    "    # Get html for the season\n",
    "\n",
    "    html = getHtml(f'https://bachelor-nation.fandom.com{season}')\n",
    "\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "    bachelor = soup.find('div', attrs={'data-source':'bachelor'}).find('a').get('href')\n",
    "\n",
    "    # Get html for the bachelor\n",
    "\n",
    "    bhtml = getHtml(f'https://bachelor-nation.fandom.com{bachelor}')\n",
    "\n",
    "    bSoup = BeautifulSoup(bhtml.text, 'html.parser')\n",
    "\n",
    "    bachelors['Name'].append(bSoup.find('div', attrs={'data-source':'name'}).find('div').text)\n",
    "    bachelors['Hometown'].append(bSoup.find('div', attrs={'data-source':'hometown'}).find('div').text)\n",
    "    bachelors['Occupation'].append(bSoup.find('div', attrs={'data-source':'occupation'}).find('div').text)\n",
    "\n",
    "    bachelors['Birth Year'].append(bSoup.find('div', attrs={'data-source':'born'}).find('div').text.replace(\",\", \"\").split()[2])\n",
    "    bachelors['Season'].append(seasonNum)\n",
    "\n",
    "    # Get contestants\n",
    "\n",
    "    # NOTE: Seasons 1-7 is only has regular tables\n",
    "    # NOTE: Season 8 is lacking data\n",
    "\n",
    "    # if (int(seasonNum) < 8):\n",
    "    #     tableImplContestants(soup, seasonNum)\n",
    "    # el\n",
    "    if(int(seasonNum) == 19):\n",
    "        galleryImplContestants(soup, seasonNum)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ff05f",
   "metadata": {},
   "source": [
    "### 2. Training Your Model\n",
    "In the cell seen below, write the code you need to train a linear regression model. Make sure you display the equation of the plane that best fits your chosen data at the end of your program. \n",
    "\n",
    "*Note, level 5 work trains a model using only the standard Python library and Pandas. A level 5 model is trained with at least two features, where one of the features begins as a categorical value (e.g. occupation, hometown, etc.). A level 4 uses external libraries like scikit or numpy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a87a9144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Train model here.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Don't forget to display the equation of the plane that best fits your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2b903",
   "metadata": {},
   "source": [
    "### 3. Testing Your Model\n",
    "In the cell seen below, write the code you need to test your linear regression model. \n",
    "\n",
    "*Note, a model is considered a level 5 if it achieves at least 60% prediction accuracy or achieves an RMSE of 2 weeks or less.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24f8b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343eb3f7",
   "metadata": {},
   "source": [
    "### 4. Final Answer\n",
    "\n",
    "In the first cell seen below, state the name of your predicted winner. \n",
    "In the second cell seen below, justify your prediction using an evaluation technique like RMSE or percent accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25533722",
   "metadata": {},
   "source": [
    "#### State the name of your predicted winner here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29adde2",
   "metadata": {},
   "source": [
    "#### Justify your prediction here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
